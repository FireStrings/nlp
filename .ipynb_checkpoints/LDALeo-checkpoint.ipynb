{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unidecode\n",
    "import demoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "nlp = spacy.load(\"pt\")\n",
    "nlp.Defaults.stop_words |= {\"gt\", \"&gt\", \"to\",\"uol\",\"mi\",\"budddhetg\",\"the\", \"ne\", \"vou\", \"ta\", \"via\",\"ex\", \"pq\", \"vc\",\"aa\",\"pra\",\"to\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_irrelevantes = set(stopwords.words('portuguese') + list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(tweet):\n",
    "    if \"|\" in tweet:\n",
    "        tweet = tweet.split(\"|\")[1]\n",
    "    \n",
    "    tweet = ' '.join(re.sub(r\"http\\S+\", \"\", tweet).split())\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|([0-9])\", \" \", tweet).split())\n",
    "    tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    tweet = ' '.join(re.sub(\"[\\_\\|\\.\\,\\\"\\'\\!\\?\\:\\;\\$\\-\\(\\)\\=]\", \" \", tweet).split())\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    le = list(demoji.findall(tweet))\n",
    "    for i in le:\n",
    "        tweet = tweet.replace(i, \"\")\n",
    "    \n",
    "    if tweet.startswith('rt '):\n",
    "        tweet = tweet.replace(\"rt \", \"\")\n",
    "    \n",
    "    lNewTweet = []\n",
    "    for i in tweet.split(\" \"):\n",
    "        if i not in palavras_irrelevantes and i not in STOP_WORDS:\n",
    "            lNewTweet.append(i)\n",
    "    \n",
    "    newTweet = \" \".join(lNewTweet)\n",
    "        \n",
    "    return unidecode.unidecode(newTweet.replace(\" rt \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/opt/documents.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_clean\"] = df.text.apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_ = [doc.split(\" \") for doc in df[\"text_clean\"].tolist()]\n",
    "l_text = df[\"text_clean\"].tolist()\n",
    "l_doc = [nlp(text) for text in l_text]\n",
    "\n",
    "# tokens = [token for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "registrou\n",
      "registrar\n",
      "\n",
      "\n",
      "sabado\n",
      "sabado\n",
      "\n",
      "\n",
      "covid\n",
      "covid\n",
      "\n",
      "\n",
      "totalizando\n",
      "totalizar\n",
      "\n",
      "\n",
      "registrar\n",
      "registrar\n",
      "\n",
      "\n",
      "divulgou\n",
      "divulgar\n",
      "\n",
      "\n",
      "ultimos\n",
      "ultimos\n",
      "\n",
      "\n",
      "seguido\n",
      "seguir\n",
      "\n",
      "\n",
      "patamar\n",
      "patamar\n",
      "\n",
      "\n",
      "marcou\n",
      "marcar\n",
      "\n",
      "\n",
      "indicando\n",
      "indicar\n",
      "\n",
      "\n",
      "registrou\n",
      "registrar\n",
      "\n",
      "\n",
      "apresenta\n",
      "apresentar\n",
      "\n",
      "\n",
      "registrou\n",
      "registrar\n",
      "\n",
      "\n",
      "conhecidos\n",
      "conhecido\n",
      "\n",
      "\n",
      "chegando\n",
      "chegar\n",
      "\n",
      "\n",
      "confirmados\n",
      "confirmar\n",
      "\n",
      "\n",
      "ultimos\n",
      "ultimos\n",
      "\n",
      "\n",
      "comparacao\n",
      "comparacao\n",
      "\n",
      "\n",
      "indicando\n",
      "indicar\n",
      "\n",
      "\n",
      "superou\n",
      "superar\n",
      "\n",
      "\n",
      "conhecidos\n",
      "conhecido\n",
      "\n",
      "\n",
      "imprensa\n",
      "imprensar\n",
      "\n",
      "\n",
      "consolidados\n",
      "consolidar\n",
      "\n",
      "\n",
      "feito\n",
      "fazer\n",
      "\n",
      "\n",
      "leva\n",
      "levar\n",
      "\n",
      "\n",
      "ultimos\n",
      "ultimos\n",
      "\n",
      "\n",
      "registrada\n",
      "registrar\n",
      "\n",
      "\n",
      "entenda\n",
      "entender\n",
      "\n",
      "\n",
      "usados\n",
      "usar\n",
      "\n",
      "\n",
      "analisar\n",
      "analisar\n",
      "\n",
      "\n",
      "vale\n",
      "valer\n",
      "\n",
      "\n",
      "ressaltar\n",
      "ressaltar\n",
      "\n",
      "\n",
      "estados\n",
      "estar\n",
      "\n",
      "\n",
      "levar\n",
      "levar\n",
      "\n",
      "\n",
      "arredondados\n",
      "arredondar\n",
      "\n",
      "\n",
      "facilitar\n",
      "facilitar\n",
      "\n",
      "\n",
      "dados\n",
      "dar\n",
      "\n",
      "\n",
      "calcular\n",
      "calcular\n",
      "\n",
      "\n",
      "leva\n",
      "levar\n",
      "\n",
      "\n",
      "exibe\n",
      "exibir\n",
      "\n",
      "\n",
      "estados\n",
      "estar\n",
      "\n",
      "\n",
      "conhecidos\n",
      "conhecido\n",
      "\n",
      "\n",
      "estados\n",
      "estar\n",
      "\n",
      "\n",
      "dados\n",
      "dar\n",
      "\n",
      "\n",
      "obtidos\n",
      "obtido\n",
      "\n",
      "\n",
      "trabalhar\n",
      "trabalhar\n",
      "\n",
      "\n",
      "reunir\n",
      "reunir\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in l_doc:\n",
    "    tokens = [token for token in doc]\n",
    "    new_doc = []\n",
    "    for tok in tokens:\n",
    "        if tok.pos_ == 'VERB':\n",
    "            new_doc.append(str(tok.lemma_))\n",
    "            print(tok)\n",
    "            print(tok.lemma_)\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            new_doc.append(str(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import sys\n",
    "\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "bigram = gensim.models.Phrases(token_, min_count=1, threshold=100) # higher threshold fewer phrases\n",
    "trigram = gensim.models.Phrases(bigram[token_], threshold=100)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [trigram_mod[bigram_mod[doc]] for doc in token_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(registrou, 'registrar'),\n",
       " (sabado, 'sabado'),\n",
       " (registrar, 'registrar'),\n",
       " (divulgou, 'divulgar'),\n",
       " (registrou, 'registrar'),\n",
       " (alta, 'alto'),\n",
       " (registrou_casos_conhecidos, 'registrou_casos_conhecidos'),\n",
       " (covid_horas_chegando, 'covid_horas_chegando'),\n",
       " (comparacao_media_dias_variacao, 'comparacao_media_dias_variacao'),\n",
       " (media_mortes_ultimos, 'media_mortes_ultimos'),\n",
       " (covid_casos_conhecidos, 'covid_casos_conhecidos'),\n",
       " (detalhes_analises_dados, 'detalhes_analises_dados'),\n",
       " (paulo_passaram_trabalhar, 'paulo_passaram_trabalhar')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, token.lemma_) for token in doc if token.pos_ == 'VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "id2word = corpora.Dictionary(l)\n",
    "corpus = [id2word.doc2bow(text) for text in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=28, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=0.31,\n",
    "                                           eta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint# Print the Keyword in the 10 topics\n",
    "# doc_lda = lda_model[corpus]\n",
    "# print(dir(doc_lda))\n",
    "# print(doc_lda.corpus)\n",
    "pprint(lda_model.print_topics())\n",
    "# pprint(dir(lda_model))\n",
    "# pprint(lda_model.get_topics())\n",
    "# print(lda_model.show_topic(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = []\n",
    "for i, row in enumerate(lda_model[corpus]):\n",
    "    row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "    lt.append(row[0][0])\n",
    "\n",
    "# print(lt)\n",
    "contents = pd.Series(lt, name=\"topic\")\n",
    "data_new = pd.concat([df, contents], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new[data_new[\"topic\"] == 22].sample(100).text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from gensim.models import CoherenceModel\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=5):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in tqdm(range(start, limit, step)):\n",
    "        \n",
    "        model = gensim.models.LdaMulticore(corpus, id2word=id2word, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    \n",
    "    return model_list, coherence_values# Colocando parametros na função\n",
    "    \n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=l, start=2, limit=30, step=2)\n",
    "# Mostrando visualmente a quantidade de tópicos\n",
    "limit=30; start=2; step=2;\n",
    "x = range(start, limit, step)\n",
    "\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Tópicos\")\n",
    "plt.ylabel(\"Score de Coerência\")\n",
    "plt.legend((\"Valores de Coerência\"), loc='best')\n",
    "plt.show()# Lista dos valores de coerência, para melhor identificar o ponto de inflexão do gráfico\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"A quantidade de tópicos =\", m, \" tem um valor de coerência de \", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new.topic.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_new[data_new[\"topic\"] == 7].head().text.tolist():\n",
    "    print(i)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import os\n",
    "import pyLDAvis# Visualize the topics\n",
    "num_topics = 28\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "    pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "    \n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages to store and manipulate data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# model building package\n",
    "import sklearn\n",
    "import re\n",
    "import unidecode\n",
    "import demoji\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "\n",
    "nlp = spacy.load(\"pt\")\n",
    "nlp.Defaults.stop_words |= {\"gt\", \"to\",\"uol\",\"mi\",\"budddhetg\",\"the\", \"ne\", \"vou\", \"ta\", \"via\",\"ex\", \"pq\", \"vc\",\"aa\",\"pra\",\"to\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"}\n",
    "palavras_irrelevantes = set(stopwords.words('portuguese') + list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/Article.csv')\n",
    "data = pd.read_csv(\"data/tweeterLeo/dataTweeter.csv\", sep=\";\", encoding='utf8')\n",
    "\n",
    "def cleanText(tweet):\n",
    "    if \"|\" in tweet:\n",
    "        tweet = tweet.split(\"|\")[1]\n",
    "        \n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|([0-9])\", \" \", tweet).split())\n",
    "    tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    tweet = ' '.join(re.sub(\"[\\_\\|\\.\\,\\\"\\'\\!\\?\\:\\;\\$\\-\\(\\)\\=]\", \" \", tweet).split())\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    le = list(demoji.findall(tweet))\n",
    "    for i in le:\n",
    "        tweet = tweet.replace(i, \"\")\n",
    "    \n",
    "    if tweet.startswith('rt '):\n",
    "        tweet = tweet.replace(\"rt \", \"\")\n",
    "    \n",
    "    lNewTweet = []\n",
    "    for i in tweet.split(\" \"):\n",
    "        if i not in palavras_irrelevantes and i not in STOP_WORDS:\n",
    "            lNewTweet.append(i)\n",
    "    \n",
    "    newTweet = \" \".join(lNewTweet)\n",
    "        \n",
    "    return unidecode.unidecode(newTweet.replace(\" rt \", \"\"))\n",
    "\n",
    "data[\"tweet_text_clean\"] = data.text.apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "cv = CountVectorizer(max_df=0.95, min_df=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtm = cv.fit_transform(df['Article'])\n",
    "dtm = cv.fit_transform(data[\"tweet_text_clean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2869x4016 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 23496 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "LDA = LatentDirichletAllocation(n_components=5,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.20000352  0.20000473  2.19756117 ...  1.20013228  0.2000115\n",
      "   0.22831849]\n",
      " [ 0.20000316  0.20039495  0.20000802 ...  1.19984747  1.20046813\n",
      "   2.15525342]\n",
      " [14.19946457  0.2000041   2.20108195 ...  0.20000711  0.72361353\n",
      "   0.21639297]\n",
      " [ 0.20012738  2.19959188  0.20000613 ...  0.20000595  0.20075823\n",
      "   0.20001545]\n",
      " [ 0.20040138  0.20000434  0.20134273 ...  0.20000719  2.67514861\n",
      "   0.20001967]]\n"
     ]
    }
   ],
   "source": [
    "# print(len(LDA.components_),type(LDA.components_))\n",
    "print(LDA.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_topic = LDA.components_[0]\n",
    "top_10_words = single_topic.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maguito\n",
      "eua\n",
      "milhoes\n",
      "mega\n",
      "homem\n",
      "medico\n",
      "mulher\n",
      "rio\n",
      "covid\n",
      "gt\n"
     ]
    }
   ],
   "source": [
    "for index in top_10_words:\n",
    "    print(cv.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['anos', 'vivo', 'sena', 'vilela', 'paulo', 'maguito', 'eua', 'milhoes', 'mega', 'homem', 'medico', 'mulher', 'rio', 'covid', 'gt']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['maia', 'senado', 'casa', 'trump', 'pandemia', 'eua', 'vacinacao', 'ouca', 'video', 'sp', 'aprova', 'gt', 'covid', 'bolsonaro', 'camara']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['pessoas', 'suspeito', 'caso', 'preso', 'prisao', 'reveillon', 'justica', 'covid', 'ano', 'governo', 'policia', 'sp', 'gt', 'rio', 'anos']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['alta', 'reino', 'unido', 'dias', 'interior', 'mulher', 'coronavac', 'anos', 'mortes', 'vacina', 'brasil', 'morre', 'gt', 'sp', 'covid']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['biden', 'paises', 'natal', 'bolsonaro', 'pandemia', 'governo', 'brasil', 'milhoes', 'pfizer', 'coronavirus', 'eua', 'ano', 'gt', 'vacina', 'covid']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,topic in enumerate(LDA.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2869, 5)\n",
      "[0.02002639 0.02007929 0.91975869 0.02002374 0.02011189]\n"
     ]
    }
   ],
   "source": [
    "topic_results = LDA.transform(dtm)\n",
    "print(topic_results.shape)\n",
    "print(topic_results[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.01, 0.91, 0.01, 0.01, 0.01, 0.01])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results[0].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Topic'] = topic_results.argmax(axis=1)\n",
    "# topic_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2497    russia vacinou pessoas covid pretende chegar m...\n",
       "15      comissario uniao europeia critica imagens verg...\n",
       "2375             revista time descreve pior ano ke yvbjgd\n",
       "1        paises america latina previsao crescimento qrehg\n",
       "1821    governo zera cobranca iof operacoes credito fe...\n",
       "826     biden violacao dados governo representa grave ...\n",
       "727     governo sp recebe lote milhoes doses vacina co...\n",
       "699     estudo inedito usp identifica proteina organis...\n",
       "2579    video mostra radiotelescopio desaba porto rico...\n",
       "975     vaticano uso vacinas covid etico moralmente ac...\n",
       "1133    agencia reguladora eua aprova uso emergencial ...\n",
       "2344    marilia mendonca lidera paradas clipes lives s...\n",
       "1409    reino unido vacinado quase pessoas covid irmto...\n",
       "1330    reta enem hora relaxar pesado estudos revisar ...\n",
       "2159    pazuello governo comprara vacina registro anvi...\n",
       "1195    pedidos refugio brasil despenca pandemia wgocs...\n",
       "1756    liberacao vacina eua garante aprovacao automat...\n",
       "1213     mike pence recebe dose vacina pfizer covid ng ph\n",
       "708     africa sul descobre cepa coronavirus mutacao &...\n",
       "208     ano primeiros relatos covid chefe oms pede dis...\n",
       "Name: tweet_text_clean, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data[\"Topic\" == 1][\"\"]\n",
    "data[data['Topic'] == 4][\"tweet_text_clean\"].sample(20)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
